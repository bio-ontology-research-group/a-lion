{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mowl\n",
    "mowl.init_jvm(\"10g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5209d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.datasets.base import PathDataset\n",
    "source_owl = \"../data/mouse.owl\"\n",
    "target_owl = \"../data/human.owl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.reasoning.normalize import GCI2\n",
    "from org.semanticweb.owlapi.apibinding import OWLManager\n",
    "from org.semanticweb.owlapi.model import IRI\n",
    "from org.semanticweb.owlapi.search import EntitySearcher\n",
    "\n",
    "manager = OWLManager()\n",
    "factory = manager.getOWLDataFactory()\n",
    "\n",
    "prefix = \"http://mowl/\"\n",
    "\n",
    "def createExAxiom(subc, prop, filler):\n",
    "    subc = factory.getOWLClass(IRI.create(f\"{subc}\"))\n",
    "    prop = factory.getOWLObjectProperty(IRI.create(f\"{prefix}has_annot_label\"))\n",
    "    filler = factory.getOWLClass(IRI.create(f\"{prefix}{filler}\"))\n",
    "\n",
    "    axiom = factory.getOWLSubClassOfAxiom(\n",
    "                subc, factory.getOWLObjectSomeValuesFrom(\n",
    "                    prop, filler))\n",
    "    return GCI2(axiom)\n",
    "           \n",
    "def format_value(value):\n",
    "    value = value.lower()\n",
    "    value = value.replace(\" \", \"_\")\n",
    "    return value\n",
    "\n",
    "def getAnnotationsAsAxioms(ont_path_file):\n",
    "    ds = PathDataset(ont_path_file)\n",
    "    ont = ds.ontology\n",
    "    classes = ont.getClassesInSignature()\n",
    "    \n",
    "    #Get annotations per class\n",
    "    annots = {}\n",
    "    for c in classes:\n",
    "        annot = EntitySearcher.getAnnotations(c, ont)\n",
    "        annots[c.toStringID()] = annot\n",
    "        \n",
    "        \n",
    "    annots_as_axioms = []\n",
    "    for k,v in annots.items():\n",
    "        for a in v:\n",
    "            if a.getValue().isLiteral() and a.getProperty().isLabel():\n",
    "                property_ = a.getProperty().toString()\n",
    "                value = str(a.getValue().asLiteral().get().getLiteral())\n",
    "                value = format_value(value)\n",
    "                annots_as_axioms.append(createExAxiom(k, property_, value))\n",
    "\n",
    "    return annots_as_axioms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as th\n",
    "from mowl.reasoning.normalize import ELNormalizer\n",
    "from normalizer import Normalizer\n",
    "from mowl.datasets.el import GCI0Dataset, GCI1Dataset, GCI2Dataset, GCI3Dataset\n",
    "from mowl.models.elboxembeddings.module import ELBoxModule\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "class ELBoxBasedModule(nn.Module):\n",
    "    def __init__(self, num_classes_src, num_rels_src, num_classes_tar, num_rels_tar, emb_size):\n",
    "        super().__init__()\n",
    "        self.num_classes_src = num_classes_src\n",
    "        self.num_classes_tar = num_classes_tar\n",
    "        self.num_rels_src = num_rels_src\n",
    "        self.num_rels_tar = num_rels_tar\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.elbox_src = ELBoxModule(\n",
    "            self.num_classes_src,\n",
    "            self.num_rels_src,\n",
    "            embed_dim = emb_size\n",
    "        )\n",
    "\n",
    "        self.elbox_tar = ELBoxModule(\n",
    "            self.num_classes_tar,\n",
    "            self.num_rels_tar,\n",
    "            embed_dim = emb_size\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*emb_size, emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, data, model = None, gci_name = None, align=False, neg = False):\n",
    "            if model == \"src\":\n",
    "                return self.elbox_src(data, gci_name, neg = neg)\n",
    "            elif model == \"tar\":\n",
    "                return self.elbox_tar(data, gci_name, neg = neg)\n",
    "            else:\n",
    "                if align:\n",
    "                    return self.fc(data)\n",
    "        \n",
    "class ELBoxModel():\n",
    "    def __init__(self, source_ont_path, target_ont_path, emb_size = 50, device = \"cpu\", lr = 0.001, epochs = 1000):\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.source_ont_path = source_ont_path\n",
    "        self.target_ont_path = target_ont_path\n",
    "        \n",
    "        self.source_ds = PathDataset(source_ont_path)\n",
    "        self.target_ds = PathDataset(target_ont_path)\n",
    "\n",
    "        self.model_filepath = \"elboxbased.th\"\n",
    "        \n",
    "        self.load_data()\n",
    "        num_classes_src = len(self.source_ds.classes) + len(list(self.src_set))\n",
    "        num_rels_src = len(self.source_ds.ontology.getObjectPropertiesInSignature()) + 1\n",
    "        num_classes_tar = len(self.target_ds.classes) + len(list(self.tar_set))\n",
    "        num_rels_tar = len(self.target_ds.ontology.getObjectPropertiesInSignature()) + 1\n",
    "        \n",
    "        self.module = ELBoxBasedModule(num_classes_src, num_rels_src, num_classes_tar, num_rels_tar, emb_size)\n",
    "\n",
    "    def load_data(self):\n",
    "        annots_ax_src = getAnnotationsAsAxioms(self.source_ont_path)\n",
    "        annots_ax_tar = getAnnotationsAsAxioms(self.target_ont_path)\n",
    "        \n",
    "        self.src_set = set([x.filler for x in annots_ax_src])\n",
    "        self.tar_set = set([x.filler for x in annots_ax_tar])\n",
    "        self.common_annots = self.src_set & self.tar_set\n",
    "        \n",
    "        normalizer = Normalizer()\n",
    "        self.source_el_gcis = normalizer.normalize(self.source_ds.ontology)\n",
    "        self.target_el_gcis = normalizer.normalize(self.target_ds.ontology)\n",
    "        \n",
    "        self.source_el_gcis[\"gci2\"] += annots_ax_src\n",
    "        self.target_el_gcis[\"gci2\"] += annots_ax_tar\n",
    "        \n",
    "        #Source\n",
    "        self.class_to_id_src = {v:k for k,v in enumerate(self.source_ds.classes + list(self.src_set))}\n",
    "        self.rel_to_id_src = {v:k for k,v in enumerate(self.source_ds.object_properties)}\n",
    "        self.rel_to_id_src[\"http://mowl/has_annot_label\"] = len(self.rel_to_id_src)\n",
    "        \n",
    "        src_gci0_dataset = GCI0Dataset(self.source_el_gcis[\"gci0\"], self.class_to_id_src, self.rel_to_id_src, device = self.device)\n",
    "        src_gci1_dataset = GCI1Dataset(self.source_el_gcis[\"gci1\"], self.class_to_id_src, self.rel_to_id_src, device = self.device)\n",
    "        src_gci2_dataset = GCI2Dataset(self.source_el_gcis[\"gci2\"], self.class_to_id_src, self.rel_to_id_src, device = self.device)\n",
    "        src_gci3_dataset = GCI3Dataset(self.source_el_gcis[\"gci3\"], self.class_to_id_src, self.rel_to_id_src, device = self.device)\n",
    "        src_gci0_bot_dataset = GCI0Dataset(self.source_el_gcis[\"gci0\"], self.class_to_id_src, self.rel_to_id_src, device = self.device)\n",
    "        src_gci1_bot_dataset = GCI1Dataset(self.source_el_gcis[\"gci1\"], self.class_to_id_src, self.rel_to_id_src, device = self.device)\n",
    "        src_gci3_bot_dataset = GCI3Dataset(self.source_el_gcis[\"gci3\"], self.class_to_id_src, self.rel_to_id_src, device = self.device)\n",
    "        \n",
    "        self.source_el_datasets = {\n",
    "            \"gci0\": src_gci0_dataset,\n",
    "            \"gci1\": src_gci1_dataset,\n",
    "            \"gci2\": src_gci2_dataset,\n",
    "            \"gci3\": src_gci3_dataset,\n",
    "            \"gci0_bot\": src_gci0_bot_dataset,\n",
    "            \"gci1_bot\": src_gci1_bot_dataset,\n",
    "            \"gci3_bot\": src_gci3_bot_dataset\n",
    "        }\n",
    "        \n",
    "        #Target\n",
    "        self.class_to_id_tar = {v:k for k,v in enumerate(self.target_ds.classes + list(self.tar_set))}\n",
    "        self.rel_to_id_tar = {v:k for k,v in enumerate(self.target_ds.object_properties)}\n",
    "        self.rel_to_id_tar[\"http://mowl/has_annot_label\"] = len(self.rel_to_id_tar)\n",
    "        \n",
    "        tar_gci0_dataset = GCI0Dataset(self.target_el_gcis[\"gci0\"], self.class_to_id_tar, self.rel_to_id_tar, device = self.device)\n",
    "        tar_gci1_dataset = GCI1Dataset(self.target_el_gcis[\"gci1\"], self.class_to_id_tar, self.rel_to_id_tar, device = self.device)\n",
    "        tar_gci2_dataset = GCI2Dataset(self.target_el_gcis[\"gci2\"], self.class_to_id_tar, self.rel_to_id_tar, device = self.device)\n",
    "        tar_gci3_dataset = GCI3Dataset(self.target_el_gcis[\"gci3\"], self.class_to_id_tar, self.rel_to_id_tar, device = self.device)\n",
    "        tar_gci0_bot_dataset = GCI0Dataset(self.target_el_gcis[\"gci0\"], self.class_to_id_tar, self.rel_to_id_tar, device = self.device)\n",
    "        tar_gci1_bot_dataset = GCI1Dataset(self.target_el_gcis[\"gci1\"], self.class_to_id_tar, self.rel_to_id_tar, device = self.device)\n",
    "        tar_gci3_bot_dataset = GCI3Dataset(self.target_el_gcis[\"gci3\"], self.class_to_id_tar, self.rel_to_id_tar, device = self.device)\n",
    "        \n",
    "        self.target_el_datasets = {\n",
    "            \"gci0\": tar_gci0_dataset,\n",
    "            \"gci1\": tar_gci1_dataset,\n",
    "            \"gci2\": tar_gci2_dataset,\n",
    "            \"gci3\": tar_gci3_dataset,\n",
    "            \"gci0_bot\": tar_gci0_bot_dataset,\n",
    "            \"gci1_bot\": tar_gci1_bot_dataset,\n",
    "            \"gci3_bot\": tar_gci3_bot_dataset\n",
    "        }\n",
    "        \n",
    "        src_annot_idxs = []\n",
    "        tar_annot_idxs = []\n",
    "        for ann in list(self.common_annots):\n",
    "            src_annot_idxs.append(self.class_to_id_src[ann])\n",
    "            tar_annot_idxs.append(self.class_to_id_tar[ann])\n",
    "        \n",
    "        self.src_annot_idxs = th.tensor(src_annot_idxs, device = self.device)\n",
    "        self.tar_annot_idxs = th.tensor(tar_annot_idxs, device = self.device)\n",
    "            \n",
    "    def get_embeddings(self):\n",
    "        src_embs = self.module.elbox_src.class_embed.weight\n",
    "        tar_embs = self.module.elbox_tar.class_embed.weight\n",
    "        \n",
    "        src_embs = {k:v for k,v in zip(self.class_to_id_src.keys(), src_embs.cpu().detach().numpy())}\n",
    "        tar_embs = {k:v for k,v in zip(self.class_to_id_tar.keys(), tar_embs.cpu().detach().numpy())}\n",
    "        \n",
    "        return src_embs, tar_embs\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        el_box_criterion = nn.MSELoss()\n",
    "        alignment_criterion = nn.BCELoss()\n",
    "        params = list(self.module.parameters())\n",
    "        optimizer = th.optim.Adam(params, lr=self.lr)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in trange(self.epochs):\n",
    "            self.module.train()\n",
    "\n",
    "            #Source space\n",
    "            loss = 0\n",
    "            for gci_name, gci_dataset in self.source_el_datasets.items():\n",
    "                if len(gci_dataset) == 0 or gci_name == \"gci0_bot\":\n",
    "                    continue\n",
    "                #Positive scores\n",
    "                rand_index = np.random.choice(len(gci_dataset), size = 512)\n",
    "                data = gci_dataset[rand_index]\n",
    "                dst = self.module(data, model = \"src\", gci_name = gci_name)\n",
    "                mse_loss = el_box_criterion(dst, th.zeros(dst.shape, requires_grad = False).to(self.device))\n",
    "                loss += mse_loss\n",
    "\n",
    "                #Negative scores\n",
    "                rand_index = np.random.choice(len(self.class_to_id_src), size = 512, replace = True)\n",
    "                rand_index = th.tensor(rand_index, device = self.device)\n",
    "                neg_data = th.cat([data[:, :-1], rand_index.unsqueeze(1)], dim = 1)\n",
    "\n",
    "                dst = self.module(neg_data, model = \"src\", gci_name = gci_name, neg = True)\n",
    "                mse_loss = el_box_criterion(dst, th.ones(dst.shape, requires_grad = False).to(self.device))\n",
    "                loss += mse_loss\n",
    "\n",
    "            #Target space\n",
    "            for gci_name, gci_dataset in self.target_el_datasets.items():\n",
    "                if len(gci_dataset) == 0 or gci_name == \"gci0_bot\":\n",
    "                    continue\n",
    "                #Positive scores\n",
    "                rand_index = np.random.choice(len(gci_dataset), size = 512)\n",
    "                data = gci_dataset[rand_index]\n",
    "                dst = self.module(data, model = \"tar\", gci_name = gci_name)\n",
    "                mse_loss = el_box_criterion(dst, th.zeros(dst.shape, requires_grad = False).to(self.device))\n",
    "                loss += mse_loss\n",
    "                #Negative scores\n",
    "                rand_index = np.random.choice(len(self.class_to_id_tar), size = 512, replace = True)\n",
    "                rand_index = th.tensor(rand_index, device = self.device)\n",
    "                neg_data = th.cat([data[:, :-1], rand_index.unsqueeze(1)], dim = 1)\n",
    "\n",
    "\n",
    "                dst = self.module(neg_data, model = \"tar\", gci_name = gci_name, neg = True)\n",
    "                mse_loss = el_box_criterion(dst, th.ones(dst.shape, requires_grad = False).to(self.device))\n",
    "                loss += mse_loss\n",
    "\n",
    "            el_box_loss = loss\n",
    "            #Alignment\n",
    "            \n",
    "            src_embs = self.module.elbox_src.class_embed(self.src_annot_idxs)\n",
    "            tar_embs = self.module.elbox_tar.class_embed(self.tar_annot_idxs)\n",
    "            embs = th.cat([src_embs, tar_embs], dim = 1)\n",
    "            \n",
    "            align_loss = self.module(embs, align = True)\n",
    "            align_loss = alignment_criterion(align_loss, th.ones(align_loss.shape, device = align_loss.device))\n",
    "            loss += align_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            el_box_loss += loss.detach().item()\n",
    "\n",
    "            checkpoint = 100\n",
    "            if best_loss > loss and (epoch+1) % checkpoint == 0:\n",
    "                best_loss = loss\n",
    "                print(\"Saving model..\")\n",
    "                th.save(self.module.state_dict(), self.model_filepath)\n",
    "            if (epoch+1) % checkpoint == 0:\n",
    "                print(f'Epoch {epoch}: ElBox loss: {el_box_loss} Align loss: {align_loss}')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELBoxModel(source_owl, target_owl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embs, tar_embs = model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from mowl.datasets.base import PathDataset\n",
    "source_owl = \"../data/mouse.owl\"\n",
    "target_owl = \"../data/human.owl\"\n",
    "\n",
    "source_ont_classes = PathDataset(source_owl).classes\n",
    "target_ont_classes = PathDataset(target_owl).classes\n",
    "\n",
    "src_to_id = {v:k for k,v in enumerate(src_embs.keys())}\n",
    "tar_to_id = {v:k for k,v in enumerate(tar_embs.keys())}\n",
    "\n",
    "src_to_id = {k:v for k,v in src_to_id.items() if k in source_ont_classes}\n",
    "tar_to_id = {k:v for k,v in tar_to_id.items() if k in target_ont_classes}\n",
    "\n",
    "scores = cosine_similarity(np.array(list(src_embs.values())), np.array(list(tar_embs.values()))) # in range [-1, 1]\n",
    "scores = (scores + 1)/2 # in range [0,1]\n",
    "with open(\"elbox_scores.tsv\", \"w\") as f:\n",
    "    f.write(\"First_Ontology_Class\\tSecond_Ontology_Class\\tScore\\tRelation\\n\")\n",
    "    for src_cls in tqdm(src_to_id, total = len(src_to_id)):\n",
    "        for tar_cls in tar_to_id:\n",
    "            src_idx = src_to_id[src_cls]\n",
    "            tar_idx = tar_to_id[tar_cls]\n",
    "            f.write(f\"{src_cls}\\t{tar_cls}\\t{scores[src_idx, tar_idx]}\\t=\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate\n",
    "avg_prec, auc = evaluate(\"elbox_scores.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_prec, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ff96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
